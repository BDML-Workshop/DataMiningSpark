{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "In order to work with Spark, we have to first set up a `SparkSession`.\n",
    "\n",
    "From this point forward, we can interact with Apache Spark using this `spark` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext(\"local[*]\",\"PySpark Word Count Example\")\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonWordCount\")\\\n",
    "        .getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this code snippet a bit further.\n",
    "In order to work with Spark, we have to set up a Spark Application which we wish to name `HelloWorldApp`.\n",
    "\n",
    "To do this:\n",
    "- We initiated a `SparkSession` using the `.builder` method.\n",
    "- We used `.appName` to tell Spark to name our Application `PythonWordCount`. \n",
    "- We used `.getOrCreate()` to tell Spark to create the Application if it does not exist yet, or reconnect to the existing app with the given name should it exist already.\n",
    "- Finally, the reference to this Spark application is stored in an object we named `spark`\n",
    "\n",
    "*__Note__ that without a SparkSession, it is not possible to access and use Spark.\n",
    "More information about SparkSession can be found [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "dataset_path=Path().resolve().parent / 'data-sets/20NewshroupDataSet/20_newsgroup/alt.atheism'\n",
    "# dataset_path=Path().resolve().parent / 'README.md'\n",
    "print(str(dataset_path.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_clean_str(x):\n",
    "  punc='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "  lowercased_str = x.lower()\n",
    "  for ch in punc:\n",
    "    lowercased_str = lowercased_str.replace(ch, ' ')\n",
    "  return lowercased_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = sc.textFile(str(dataset_path.resolve())+\"/*/*\").map(lower_clean_str)\n",
    "lines = spark.read.text(str(dataset_path.resolve())+\"/*\").rdd.map(lambda r: r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "clean_lines = lines.map(lower_clean_str)\n",
    "words = clean_lines.flatMap(lambda x: x.split(' '))\n",
    "counts_clean = words.map(lambda x: (x, 1)).reduceByKey(add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other \n",
    "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = lambda t: lambda v: v > t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(threshold(10)(20))\n",
    "print(threshold(20)(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filtered = counts_clean.filter(lambda couple: threshold(50)(couple[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = count_filtered.collect()\n",
    "for (word, count) in output:\n",
    "        print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = counts_clean.collect()\n",
    "for (word, count) in output:\n",
    "        if threshold(count ,50):\n",
    "            print(\"%s: %i\" % (word, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
