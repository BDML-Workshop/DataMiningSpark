{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitignore\n",
      "First\n",
      "LICENSE\n",
      "README.md\n",
      "requirements.txt\n",
      "requirements_notebook.txt\n",
      "requirement_me.txt\n",
      "requirement_possible.txt\n",
      "Section 0 - Python and Spark a match made in heaven\n",
      "Section 1 - MapReduce starting with Spark\n",
      "Section 2 - Working with PySpark\n",
      "Section 3 - Preparing Data using SparkSQL\n",
      "Section 4 - Machine Learning with Spark MLlib\n",
      "Section 5 - Classification and Regression\n",
      "Section 6 - Analyzing Big Data\n",
      "Section 8 - Machine Learning in Real-Time\n",
      "Section 9 - The Power Of PySpark\n",
      "Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dirs = os.listdir( \"..\")\n",
    "\n",
    "# This would print all the files and directories\n",
    "for file in dirs:\n",
    "   print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "#findspark.init('/home/pascalfares/DataMiningSpark/sparkhome/spark-3.0.1-bin-hadoop2.7')\n",
    "findspark.init('/opt/spark/spark-3.0.1-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the textFile RDD to read the README.md file (old RDD) see next section for dtaframe to rdd reading text files\n",
    "#   Note this is lazy \n",
    "spark = SparkSession.builder.appName(\"QuickStart\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "textFile = sc.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When performing an action (like a count) this is when the textFile is read and aggregate calculated\n",
    "#    Click on [View] to see the stages and executors\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Mastering Big Data Analytics with PySpark [Machine Learning & Data Mining Workshop]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(textFile.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Mastering Big Data Analytics with PySpark [Machine Learning & Data Mining Workshop]',\n",
       " 'This is the code repository for the lab the 2 first sessions \"Machine Learning & Data Mining Workshop\".',\n",
       " '',\n",
       " 'Theses hand-on are mainly inspired by this workshop : https://github.com/PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark Authored by: [Danny Meijer](https://www.linkedin.com/in/dannydatascientist). It is in fact a fork with adaptation to windows 10 and add some parts issued form our courses in [Cnam Liban](http://www.cnam-liban.fr).',\n",
       " '[Ingénierie de la fouille et de la visualisation de données massives](http://cedric.cnam.fr/vertigo/Cours/RCP216/)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all of the lines wihtin the RDD and output the first five rows\n",
    "linesWithSpark = textFile.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Mastering Big Data Analytics with PySpark [Machine Learning & Data Mining Workshop]',\n",
       " 'Theses hand-on are mainly inspired by this workshop : https://github.com/PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark Authored by: [Danny Meijer](https://www.linkedin.com/in/dannydatascientist). It is in fact a fork with adaptation to windows 10 and add some parts issued form our courses in [Cnam Liban](http://www.cnam-liban.fr).',\n",
       " \"PySpark helps you perform data analysis at-scale; it enables you to build more scalable analyses and pipelines. This course starts by introducing you to PySpark's potential for performing effective analyses of large datasets. You'll learn how to interact with Spark from Python and connect Jupyter to Spark to provide rich data visualizations. After that, you'll delve into various Spark components and its architecture.\",\n",
       " \"You'll learn to work with Apache Spark and perform ML tasks more smoothly than before. Gathering and querying data using Spark SQL, to overcome challenges involved in reading it. You'll use the DataFrame API to operate with Spark MLlib and learn about the Pipeline API. Finally, we provide tips and tricks for deploying your code and performance tuning.\",\n",
       " 'By the end of this course, you will not only be able to perform efficient data analytics but will have also learned to use PySpark to easily analyze large datasets at-scale in your organization.',\n",
       " '* Run, process, and analyze large chunks of datasets using PySpark',\n",
       " '* Utilize Spark SQL to easily load big data into DataFrames',\n",
       " '* Create fast and scalable Machine Learning applications using MLlib with Spark',\n",
       " '* Achieve scalable, high-throughput and fault-tolerant processing of data streams using Spark Streaming',\n",
       " 'There is a bundled script that will help you in all theses steps, but to be able to launch the script we need first to install python, and java (needed for Spark), you will not use Java but it is needed for running spark']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.map(lambda l: l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
