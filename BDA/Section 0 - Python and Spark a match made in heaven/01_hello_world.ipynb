{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "In order to work with Spark, we have to first set up a `SparkSession`.\n",
    "\n",
    "From this point forward, we can interact with Apache Spark using this `spark` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "#findspark.init('/home/pascalfares/DataMiningSpark/sparkhome/spark-3.0.1-bin-hadoop2.7')\n",
    "findspark.init('/opt/spark/spark-3.0.1-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The findspark module help as to init spark in a python enviroment like jupyter, the one we are working on now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The builder method is used to set up an app which we name 'HelloWorldApp'\n",
    "spark = SparkSession.builder.appName(\"HelloWorldApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this code snippet a bit further.\n",
    "In order to work with Spark, we have to set up a Spark Application which we wish to name `HelloWorldApp`.\n",
    "\n",
    "To do this:\n",
    "- We initiated a `SparkSession` using the `.builder` method.\n",
    "- We used `.appName` to tell Spark to name our Application `HelloWorldApp`. \n",
    "- We used `.getOrCreate()` to tell Spark to create the Application if it does not exist yet, or reconnect to the existing app with the given name should it exist already.\n",
    "- Finally, the reference to this Spark application is stored in an object we named `spark`\n",
    "\n",
    "*__Note__ that without a SparkSession, it is not possible to access and use Spark.\n",
    "More information about SparkSession can be found [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World\n",
    "\n",
    "Next, we will use this newly created `spark` object to create some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the textFile RDD to read the README.md file (old RDD) see next section for dtaframe to rdd reading text files\n",
    "#   Note this is lazy \n",
    "\n",
    "textFile = spark.sparkContext.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When performing an action (like a count) this is when the textFile is read and aggregate calculated\n",
    "#    Click on [View] to see the stages and executors\n",
    "\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Mastering Big Data Analytics with PySpark [Machine Learning & Data Mining Workshop]',\n",
       " 'This is the code repository for the lab the 2 first sessions \"Machine Learning & Data Mining Workshop\".',\n",
       " '',\n",
       " 'Theses hand-on are mainly inspired by this workshop : https://github.com/PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark Authored by: [Danny Meijer](https://www.linkedin.com/in/dannydatascientist). It is in fact a fork with adaptation to windows 10 and add some parts issued form our courses in [Cnam Liban](http://www.cnam-liban.fr).',\n",
       " '[Ingénierie de la fouille et de la visualisation de données massives](http://cedric.cnam.fr/vertigo/Cours/RCP216/)',\n",
       " 'and',\n",
       " '[Cours de bases de données documentaires et distribuées](http://b3d.bdpedia.fr/)',\n",
       " '',\n",
       " '',\n",
       " '## About the WorkShop',\n",
       " '',\n",
       " '[ ] adapt this paragraph',\n",
       " '',\n",
       " \"PySpark helps you perform data analysis at-scale; it enables you to build more scalable analyses and pipelines. This course starts by introducing you to PySpark's potential for performing effective analyses of large datasets. You'll learn how to interact with Spark from Python and connect Jupyter to Spark to provide rich data visualizations. After that, you'll delve into various Spark components and its architecture.\",\n",
       " '',\n",
       " \"You'll learn to work with Apache Spark and perform ML tasks more smoothly than before. Gathering and querying data using Spark SQL, to overcome challenges involved in reading it. You'll use the DataFrame API to operate with Spark MLlib and learn about the Pipeline API. Finally, we provide tips and tricks for deploying your code and performance tuning.\",\n",
       " '',\n",
       " 'By the end of this course, you will not only be able to perform efficient data analytics but will have also learned to use PySpark to easily analyze large datasets at-scale in your organization.',\n",
       " '',\n",
       " '## What You Will Learn',\n",
       " '',\n",
       " '* Gain knowledge of vital Data Analytics concepts via practical use cases',\n",
       " '* Create elegant data visualizations using Jupyter',\n",
       " '* Run, process, and analyze large chunks of datasets using PySpark',\n",
       " '* Utilize Spark SQL to easily load big data into DataFrames',\n",
       " '* Create fast and scalable Machine Learning applications using MLlib with Spark',\n",
       " '* Perform exploratory Data Analysis in a scalable way',\n",
       " '* Achieve scalable, high-throughput and fault-tolerant processing of data streams using Spark Streaming',\n",
       " '',\n",
       " '',\n",
       " '## Instructions and Navigation',\n",
       " '### Assumed Knowledge',\n",
       " 'This course will greatly appeal to data science enthusiasts, data scientists, or anyone who is familiar with Machine Learning concepts and wants to scale out his/her work to work with big data.',\n",
       " '',\n",
       " 'If you find it difficult to analyze large datasets that keep growing, then theses two first sessions are  the perfect guide for you!',\n",
       " '',\n",
       " 'A working knowledge of Python, installing and manipulatng some systeme admin in wondows 10 or Linux assumed.',\n",
       " '',\n",
       " '## Technical Requirements ',\n",
       " '#### Minimum Hardware Requirements',\n",
       " 'For successful completion of theses sessions, students will require the computer systems with at least the following:',\n",
       " '',\n",
       " 'OS: Windows entreprise or professional 10 build > 1903, any Linux distro (preferred), Mac (be autonome)',\n",
       " 'Processor: Any processor from the last few years',\n",
       " 'Memory: 2GB RAM',\n",
       " 'Storage: 300MB for the Integrated Development Environment (IDE) and 1GB for cache',\n",
       " '',\n",
       " '#### Recommended Hardware Requirements ',\n",
       " 'For an optimal experience with hands-on labs and other practical activities, we recommend the following configuration:',\n",
       " '',\n",
       " 'OS: Windows entreprise or professional 10 build > 1903,  or Mac',\n",
       " 'Processor: Core i5 or better (or AMD equivalent)',\n",
       " 'Memory: 8GB RAM or better',\n",
       " 'Storage:  2GB free for build caches and dependencies',\n",
       " '',\n",
       " '#### Software Requirements',\n",
       " '',\n",
       " 'Operating system: Windows 10, Linux or mac',\n",
       " '',\n",
       " '',\n",
       " '## Follow the instructions below to download the data belonging to the course as well as',\n",
       " '',\n",
       " '### setting up your interactive development environment.',\n",
       " '',\n",
       " 'There is a bundled script that will help you in all theses steps, but to be able to launch the script we need first to install python, and java (needed for Spark), you will not use Java but it is needed for running spark',\n",
       " '',\n",
       " '#### We will use powershell, verify that it is installed on your windows 10',\n",
       " '',\n",
       " '#### Step1 : Install Java',\n",
       " '',\n",
       " 'First verify if java installed, if not install it',\n",
       " '',\n",
       " '#### Step 2 : Install python ',\n",
       " '',\n",
       " 'First verify that python is installed, if not install it',\n",
       " '',\n",
       " '#### Step 3: install git',\n",
       " '',\n",
       " '### Downloading Data for this Course',\n",
       " '',\n",
       " 'Once you have cloned this repository locally, simply navigate to the folder you have',\n",
       " ' stored the repo in and run: ',\n",
       " ' ',\n",
       " ' #### in the folder <repoFolder>/BDA',\n",
       " '```python download_and_install_pyspark.py```',\n",
       " '',\n",
       " 'This will install an environnement to be used on a shell or powershell terminal and jupyter lab (will use to simplify things only jupyter lab)',\n",
       " '',\n",
       " '#### in the folder <repoFolder>/BDA',\n",
       " '```python download_data.py```',\n",
       " '',\n",
       " 'This will populate the `data-sets` folder in your repo with a number of data sets that',\n",
       " ' will be used throughout the course.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '### Instructions for use',\n",
       " '',\n",
       " 'There are 2 ways to access jupyter lab if theses two sessions:',\n",
       " '1. Through the bundled `run_me.py` script (recommended to use)',\n",
       " '2. Through the CLI (only for advanced users)',\n",
       " '',\n",
       " '#### Using the bundled script to run jupyter lab',\n",
       " '',\n",
       " 'The easiest way to run the container that belongs to this two sessions is by running',\n",
       " ' ```python run_me.py``` from the git repository. This will automatically',\n",
       " ' install all necessary softwares and python modules for you, set up envirement variables, download the data, and launch jupyter lab',\n",
       " '',\n",
       " '#### Using CLI',\n",
       " '',\n",
       " '[ ] TODO',\n",
       " '',\n",
       " '',\n",
       " '#### To Stop the session',\n",
       " '',\n",
       " '[ ] TODO',\n",
       " '']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = textFile.collect()\n",
    "display(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|         c1|\n",
      "+-----------+\n",
      "|hello world|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Spark SQL, we create a dataframe which holds our `hello world` data\n",
    "df = spark.sql('SELECT \"hello world\" as c1')\n",
    "\n",
    "# We can then use the `show()` method to see what the DataFrame we just created looks like\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything right, you should see a table with your Hello World message inside. __Congratulations!__ You've just built your first Spark application that says hello to the world!!\n",
    "\n",
    "> *__Troubleshooting__: if you run this code snippet without having set up a SparkSession (Spark Application), it throws an error like this:*\n",
    "> ```\n",
    "Py4JJavaError: An error occurred while calling o116.showString.\n",
    ": java.lang.IllegalStateException: SparkContext has been shutdown\n",
    "```\n",
    "> ->\n",
    "> __Fix this by running the SparkSession builder first (cell above)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping our Spark application\n",
    "\n",
    "It is always good practice to clean up behind ourselves. \n",
    "As we do not need this Application anymore after running what we want from it, we can kill it (stop it) using `.stop()`.\n",
    "This tells Spark that it can kill off this Application and free up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To kill the Spark application, use the `stop()` method\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That brings us to the end of this part of our tutorial.\n",
    "\n",
    "**Happy Sparking!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lesson 1:** What is RDD, Dataframe and dataSet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark RDD APIs – An RDD stands for Resilient Distributed Datasets. It is Read-only partition collection of records. RDD is the fundamental data structure of Spark. It allows a programmer to perform in-memory computations on large clusters in a fault-tolerant manner. Thus, speed up the task.\n",
    "* Spark Dataframe APIs – Unlike an RDD, data organized into named columns. For example a table in a relational database. It is an immutable distributed collection of data. DataFrame in Spark allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction.\n",
    "* Spark Dataset APIs – Datasets in Apache Spark are an extension of DataFrame API which provides type-safe, object-oriented programming interface. Dataset takes advantage of Spark’s Catalyst optimizer by exposing expressions and data fields to a query planner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
